{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Class Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise we'll use logistic regression to recognize hand-written digits (0 to 9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__header__': b'MATLAB 5.0 MAT-file, Platform: GLNXA64, Created on: Sun Oct 16 13:09:09 2011',\n",
       " '__version__': '1.0',\n",
       " '__globals__': [],\n",
       " 'X': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
       " 'y': array([[10],\n",
       "        [10],\n",
       "        [10],\n",
       "        ...,\n",
       "        [ 9],\n",
       "        [ 9],\n",
       "        [ 9]], dtype=uint8)}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "data = loadmat('/Users/natalychacon/Documents/Machine Learning/std coursera/machine-learning-ex/ex3/ex3data1.mat') \n",
    "#As ex3data1 is in Matlab native format loadmat had to be used.\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 5000 training examples in ex3data1.mat, where each training example is a 20 pixel by 20 pixel grayscale image of the digit. Each pixel is represented by a floating point number indicating the grayscale intensity at that location. The 20 by 20 grid of pixels is 'unrolled' into a 400-dimensional vector. Each of these training examples becomes a single row in our data matrix X. This gives us a 5000 by 400 matrix X where every row is a training example for a handwritten digit image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 400), (5000, 1))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dimentions of the data\n",
    "data['X'].shape, data['y'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rows = data['X'].shape[0]\n",
    "X = np.insert(data['X'], 0, values=np.ones(rows), axis=1)\n",
    "X1 = X[5]\n",
    "X1 = X1[0:400]\n",
    "X1 = np.array(X1)\n",
    "X1 = X1.reshape(20,20)\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(X1, cmap=plt.cm.gray, interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(theta,X,y,learning_rate):\n",
    "    m = np.size(y,0)\n",
    "    theta = np.matrix(theta)\n",
    "    X = np.matrix(X)\n",
    "    y = np.matrix(y)\n",
    "    sig = sigmoid(X * theta.T)\n",
    "    j = 1/m * np.sum(np.multiply(-y,np.log(sig))-np.multiply((1-y),np.log(1-sig))) + (((learning_rate)/(2*m))*np.sum(np.power(theta[:,1:theta.shape[1]],2)))\n",
    "    return j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(theta,X,y,learning_rate):\n",
    "    theta = np.matrix(theta)\n",
    "    X = np.matrix(X)\n",
    "    y = np.matrix(y)\n",
    "    m = np.size(y,0)\n",
    "    sig = sigmoid(X * theta.T)\n",
    "    grad1 = 1/m *X.T*(sig-y) \n",
    "    grad2 = learning_rate/m * theta\n",
    "    grad2[0,0]=0\n",
    "    return grad1.T+grad2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_t = np.matrix('-2, -1, 1, 2')\n",
    "X_t = np.insert(np.reshape(np.arange(1,16), (5,3), order='F')/10, 0, 1, axis=1)\n",
    "y_t = np.matrix(\"1;0;1;0;1\")\n",
    "lambda_t = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.14656137, -0.54855841,  0.72472227,  1.39800296]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient(theta_t,X_t,y_t,lambda_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.534819396109744"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost(theta_t,X_t,y_t,lambda_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-vs-all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've defined our cost and gradient functions, it's time to build a classifier. For this task we've got 10 possible classes, and since logistic regression is only able to distiguish between 2 classes at a time, we need a strategy to deal with the multi-class scenario. In this exercise we're tasked with implementing a one-vs-all classification approach, where a label with k different classes results in k classifiers, each one deciding between \"class i\" and \"not class i\" (i.e. any class other than i). We're going to wrap the classifier training up in one function that computes the final weights for each of the 10 classifiers and returns the weights as a k X (n + 1) array, where n is the number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "def one_vs_all(X,y,num_labels,learning_rate):\n",
    "    m = X.shape[0] #Rows\n",
    "    n = X.shape[1] #Parameters\n",
    "    \n",
    "    all_theta = np.zeros((num_labels, n + 1))\n",
    "    \n",
    "    #insert a column of ones for the intercept therm\n",
    "    X = np.insert(X, 0, 1, axis=1)\n",
    "   \n",
    "   \n",
    "    # labels are 1-indexed instead of 0-indexed\n",
    "    for i in range(1, num_labels + 1):\n",
    "        theta = np.zeros(n + 1)\n",
    "        y_i = np.array([1 if label == i else 0 for label in y])\n",
    "        y_i = np.reshape(y_i, (m, 1))\n",
    "        \n",
    "        # minimize the objective function\n",
    "        fmin = minimize(fun=cost, x0=theta, args=(X, y_i, learning_rate), method='TNC', jac=gradient)\n",
    "        all_theta[i-1,:] = fmin.x\n",
    "    \n",
    "    return all_theta\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are transforming y from  class label to a binary value for each classifier. Spicy minimize is used to minimize the cost function for each classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.38161327e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         1.30425774e-03, -7.18023833e-10,  0.00000000e+00],\n",
       "       [-3.18368854e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         4.45819726e-03, -5.08285164e-04,  0.00000000e+00],\n",
       "       [-4.79819385e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "        -2.86238044e-05, -2.46864927e-07,  0.00000000e+00],\n",
       "       ...,\n",
       "       [-7.98669723e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "        -8.95615484e-05,  7.22105352e-06,  0.00000000e+00],\n",
       "       [-4.57192130e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "        -1.33570237e-03,  9.98944016e-05,  0.00000000e+00],\n",
       "       [-5.40479660e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "        -1.16598595e-04,  7.88728728e-06,  0.00000000e+00]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_theta = one_vs_all(data['X'], data['y'], 10, 1)\n",
    "all_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to used the trained classifiers to predict a label for each image.We're going to compute the class probability for each class, for each training instance (using vectorized code of course!) and assign the output class label as the class with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_One_vs_all (X,all_theta):\n",
    "    X = np.matrix(X)\n",
    "    all_theta = np.matrix(all_theta)\n",
    "    m = X.shape[0] #Rows\n",
    "    num_labels = np.size(theta_t,0)\n",
    "    \n",
    "    #insert a column of ones for the intercept therm\n",
    "    X = np.insert(X, 0, 1, axis=1)\n",
    "    \n",
    "    # compute the class probability for each class on each training instance\n",
    "    A = X * all_theta.T\n",
    "    \n",
    "    # create array of the index with the maximum probability\n",
    "    A_argmax = np.argmax(A, axis=1)\n",
    "    \n",
    "    return A_argmax    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[9],\n",
       "        [9],\n",
       "        [9],\n",
       "        ...,\n",
       "        [8],\n",
       "        [8],\n",
       "        [6]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_One_vs_all(data['X'], all_theta)\n",
    "y_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
